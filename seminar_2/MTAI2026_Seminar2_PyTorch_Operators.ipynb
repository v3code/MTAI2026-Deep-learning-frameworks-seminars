{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "SY_jWpBGxhiC"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Seminar 2. Custom PyTorch Operators",
   "metadata": {
    "id": "S9W6xA6Cw2fD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Models in PyTorch Through Composition\n",
    "\n",
    "PyTorch models are built using **composition**.  \n",
    "Instead of defining one large monolithic network, we construct models by combining smaller, reusable modules.\n",
    "\n",
    "Each module can contain other modules, which allows us to build hierarchical and well-structured architectures.\n",
    "\n",
    "---\n",
    "\n",
    "## Composition\n",
    "\n",
    "Composition means:\n",
    "\n",
    "- A model is built from smaller blocks.\n",
    "- Each block can contain multiple layers.\n",
    "- Blocks can be reused in larger architectures.\n",
    "- Complex models are created by stacking simpler components.\n",
    "\n",
    "This keeps code:\n",
    "\n",
    "- Modular  \n",
    "- Reusable  \n",
    "- Readable  \n",
    "- Easy to extend  \n",
    "\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- Inherit from `nn.Module`\n",
    "- Define layers inside `__init__`\n",
    "- Define computation in `forward()`\n",
    "- Create reusable blocks\n",
    "- Build larger models by combining blocks\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Model Built from Two Blocks\n",
    "\n",
    "Below is a simple example where:\n",
    "\n",
    "- We define a reusable blocks: `LinearReLUBlock` and `LinearTanhBlock`\n",
    "- The final model is composed of two such blocks\n"
   ],
   "metadata": {
    "id": "TrtRA_a9w7fk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-k1k7CPw2Ac"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearReLUBlock(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearTanhBlock(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = LinearReLUBlock(4, 8)\n",
    "        self.block2 = LinearTanhBlock(8, 8)\n",
    "        self.output = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CombinedModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What `nn.Module` Enables\n",
    "\n",
    "When we inherit from `nn.Module`, we automatically gain powerful functionality that works **recursively across all submodules**.\n",
    "\n",
    "## What `nn.Module` Gives Us\n",
    "\n"
   ],
   "metadata": {
    "id": "N2JKkprm0p0_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter Registration\n",
    "\n",
    "All layers assigned as attributes (e.g. `self.linear = nn.Linear(...)`) are:\n",
    "\n",
    "- Automatically registered\n",
    "- Collected in `model.parameters()`\n",
    "- Included in `model.state_dict()`\n",
    "\n",
    "This works **recursively** for all sub-blocks."
   ],
   "metadata": {
    "id": "PfJmcu-Q03GO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Registered parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ],
   "metadata": {
    "id": "YieDt7Es07QQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automatic Gradient Tracking\n",
    "\n",
    "During the forward pass:\n",
    "\n",
    "- PyTorch dynamically builds a computation graph\n",
    "- Calling `loss.backward()` computes gradients\n",
    "- Gradients are stored in each parameter’s `.grad`\n",
    "\n",
    "No manual graph management is required."
   ],
   "metadata": {
    "id": "JwAzSi3V09tV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(5, 4)\n",
    "target = torch.randn(5, 2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "output = model(x)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradient computed for output layer:\",\n",
    "      model.output.weight.grad is not None)\n",
    "model.output.weight.grad"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "v3ukZbwD1GYD",
    "outputId": "543c3a8f-532d-4d46-a777-eea4cc6d621d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-2298907601.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mcriterion\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMSELoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Device and Type Transfer (`.to()`)\n",
    "\n",
    "Calling:\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "or\n",
    "\n",
    "    model.to(dtype)\n",
    "\n",
    "moves all:\n",
    "\n",
    "- Parameters\n",
    "- Buffers\n",
    "- Submodules\n",
    "\n",
    "to CPU/GPU/dtype automatically."
   ],
   "metadata": {
    "id": "nWZdB_sW1DgP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "model = model.to(device=device, dtype=dtype)\n",
    "\n",
    "x: torch.Tensor = torch.randn(5, 4, device=device, dtype=dtype)\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype)"
   ],
   "metadata": {
    "id": "OXVzg8_c1roi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving & Loading (`state_dict()`)\n",
    "\n",
    "- `model.state_dict()` returns all parameters recursively\n",
    "- `model.load_state_dict(...)` restores them\n",
    "\n",
    "This works across the full module tree."
   ],
   "metadata": {
    "id": "8suS5_Jl1_7h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, \"combined_model.pt\")"
   ],
   "metadata": {
    "id": "OvJHOmeP2F8o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `train()` vs `eval()` Mode in PyTorch\n",
    "\n",
    "PyTorch modules have two main modes: **training mode** and **evaluation mode**.  \n",
    "Switching between them affects layers that behave differently during training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## `model.train()`\n",
    "\n",
    "- Sets the model to **training mode**.\n",
    "- Used when training the model with gradient updates.\n",
    "- Affects certain layers, such as:\n",
    "\n",
    "| Layer Type        | Behavior in `train()` Mode                  |\n",
    "|------------------|--------------------------------------------|\n",
    "| `Dropout`         | Randomly zeroes some activations           |\n",
    "| `BatchNorm`       | Updates running statistics (mean/variance) |\n",
    "\n",
    "- Gradients are computed as usual.\n",
    "\n",
    "---\n",
    "\n",
    "## `model.eval()`\n",
    "\n",
    "- Sets the model to **evaluation (inference) mode**.\n",
    "- Used when evaluating or deploying the model.\n",
    "- Affects certain layers:\n",
    "\n",
    "| Layer Type        | Behavior in `eval()` Mode                   |\n",
    "|------------------|--------------------------------------------|\n",
    "| `Dropout`         | Passes all activations through unchanged  |\n",
    "| `BatchNorm`       | Uses stored running mean/variance         |\n",
    "\n",
    "- No layers update internal statistics.\n",
    "- Gradients are usually not required (often used with `torch.no_grad()`).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- Always use `model.train()` during training.\n",
    "- Always use `model.eval()` during evaluation or testing.\n",
    "- Forgetting to switch can lead to inconsistent results, especially with `Dropout` or `BatchNorm`.\n",
    "\n"
   ],
   "metadata": {
    "id": "4MRq97X539iv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple model with Dropout and BatchNorm\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)\n",
    "        self.bn = nn.BatchNorm1d(8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleModel()\n",
    "x = torch.randn(5, 4)\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "output_train = model(x)\n",
    "print(\"Training mode output:\\n\", output_train)\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_eval = model(x)\n",
    "print(\"Evaluation mode output:\\n\", output_eval)\n"
   ],
   "metadata": {
    "id": "AEJprWpX388f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `torch.no_grad()` and `torch.inference_mode()` in PyTorch\n",
    "\n",
    "When performing inference (evaluating a model without updating parameters), PyTorch provides context managers to **disable gradient tracking**. This saves memory and speeds up computation.\n",
    "\n",
    "---\n",
    "\n",
    "## `torch.no_grad()`\n",
    "\n",
    "- Disables gradient tracking.\n",
    "- Useful during evaluation or inference.\n",
    "- Gradients are **not computed**, but autograd still tracks operations for some internal purposes.\n",
    "- Can be used as a **context manager** or a **function decorator**.\n",
    "\n",
    "---\n",
    "\n",
    "## `torch.inference_mode()`\n",
    "\n",
    "- Introduced in PyTorch 1.9.\n",
    "- Similar to `no_grad()`, but **more efficient**.\n",
    "- Completely disables autograd and reduces memory usage.\n",
    "- Recommended for pure inference pipelines.\n",
    "\n"
   ],
   "metadata": {
    "id": "Im6UqmJJ4hvK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(4, 2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Using torch.no_grad() as method decorator\n",
    "    # -----------------------------\n",
    "    @torch.no_grad()\n",
    "    def forward_no_grad(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Using torch.inference_mode() as method decorator\n",
    "    # -----------------------------\n",
    "    @torch.inference_mode()\n",
    "    def forward_inference(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = SimpleModel()\n",
    "x = torch.randn(5, 4)\n",
    "\n",
    "# -----------------------------\n",
    "# Call decorated methods\n",
    "# -----------------------------\n",
    "output_no_grad_method = model.forward_no_grad(x)\n",
    "output_infer_method = model.forward_inference(x)\n",
    "\n",
    "print(\"Output no_grad method:\\n\", output_no_grad_method)\n",
    "print(\"Output inference_mode method:\\n\", output_infer_method)\n",
    "\n",
    "# -----------------------------\n",
    "# Using context managers\n",
    "# -----------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_no_grad_cm = model(x)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_infer_cm = model(x)\n",
    "\n",
    "print(\"Output no_grad context manager:\\n\", output_no_grad_cm)\n",
    "print(\"Output inference_mode context manager:\\n\", output_infer_cm)\n"
   ],
   "metadata": {
    "id": "ZqUP-i0a4yCU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Disabling Gradients with `requires_grad_(False)`\n",
    "\n",
    "PyTorch provides a convenient method `requires_grad_()` that can **enable or disable gradients in-place** for all parameters of a model or a tensor.\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "param.requires_grad_(False)\n",
    "```\n",
    "\n",
    "- Sets `requires_grad=False` **in-place** for that parameter.\n",
    "- This is useful for freezing models during inference or transfer learning.\n",
    "- Can be applied to an entire model recursively by iterating over its parameters.\n"
   ],
   "metadata": {
    "id": "-txVuqp55FSM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = SimpleModel()\n",
    "\n",
    "# Disable gradient computation for all parameters using requires_grad_()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# Verify\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Forward pass still works\n",
    "x = torch.randn(5, 4)\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)"
   ],
   "metadata": {
    "id": "Ah23Xi1q5vsh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Redefining `train()` and `eval()` in `nn.Module`\n",
    "\n",
    "PyTorch’s `nn.Module` provides built-in `train(mode: bool = True)` and `eval()` methods to switch between **training** and **evaluation** modes.  \n",
    "\n",
    "Sometimes, when creating **custom modules or blocks**, you might want to **override these methods** to perform extra actions whenever the mode changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Override?\n",
    "\n",
    "- Apply mode-specific logic to sub-blocks or attributes that are not standard layers\n",
    "- Log or track mode switches\n",
    "- Automatically modify internal flags or buffers along with training/eval mode\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- `train(mode: bool = True)` sets `self.training = mode` for the module\n",
    "- `eval()` is equivalent to `train(False)`\n",
    "- Default implementation recursively calls `train(mode)` on all submodules\n",
    "- Overriding allows custom behavior while keeping recursive updates intact\n"
   ],
   "metadata": {
    "id": "U__oHhIH5xZv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Self\n",
    "\n",
    "class CustomBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 4)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    # -----------------------------\n",
    "    # Override train() method\n",
    "    # -----------------------------\n",
    "    def train(self, mode: bool = True) -> Self:\n",
    "        print(f\"CustomBlock set to {'train' if mode else 'eval'} mode\")\n",
    "        super().train(mode)  # Call original method to update submodules\n",
    "        # Add any custom logic here\n",
    "        return self\n",
    "\n",
    "    # -----------------------------\n",
    "    # Override eval() method\n",
    "    # -----------------------------\n",
    "    def eval(self) -> Self:\n",
    "        print(\"CustomBlock set to eval mode\")\n",
    "        return super().eval()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = CustomBlock()\n",
    "x = torch.randn(2, 4)\n",
    "\n",
    "# Switch to training mode\n",
    "model.train()\n",
    "output_train = model(x)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_eval = model(x)\n",
    "\n",
    "print(\"Output training mode:\", output_train)\n",
    "print(\"Output eval mode:\", output_eval)\n"
   ],
   "metadata": {
    "id": "XEr6naKI5-ZD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common Module Aggregators in PyTorch\n",
    "\n",
    "When building neural networks, it is often useful to group multiple layers or submodules together.  \n",
    "PyTorch provides several **module aggregators** that help organize layers and blocks. The most common ones are:\n",
    "\n"
   ],
   "metadata": {
    "id": "RqV2VNcz6cd3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `nn.Sequential`\n",
    "\n",
    "- Holds modules in a sequential order.\n",
    "- Executes them **in the order they are added** during the forward pass.\n",
    "- Ideal for simple **stacked layers** with a single input and output.\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "- Forward pass is automatically defined.\n",
    "- Cannot handle multiple inputs or branching."
   ],
   "metadata": {
    "id": "S9190Lr66itl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "x = torch.randn(5, 4)\n",
    "output_seq = seq_model(x)\n",
    "print(\"nn.Sequential output shape:\", output_seq.shape)\n"
   ],
   "metadata": {
    "id": "6QshfTTJ6tYY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `nn.ModuleList`\n",
    "\n",
    "- Holds a **list of modules**.\n",
    "- Does **not define a forward pass automatically**.\n",
    "- Useful when you need to **loop over modules**, or have conditional computation.\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "- Modules are registered properly, so parameters are tracked.\n",
    "- You must define your own `forward()`."
   ],
   "metadata": {
    "id": "LnA9P8sa6lHg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ModuleListModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "ml_model = ModuleListModel()\n",
    "output_ml = ml_model(x)\n",
    "print(\"nn.ModuleList output shape:\", output_ml.shape)"
   ],
   "metadata": {
    "id": "51xip2in6wte"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `nn.ModuleDict`\n",
    "\n",
    "- Holds modules in a **dictionary** with string keys.\n",
    "- Useful for architectures with **named branches**, **dynamic selection**, or **multi-head outputs**.\n",
    "- Like `ModuleList`, it does **not define a forward pass**."
   ],
   "metadata": {
    "id": "8F3PwKJs6mgf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ModuleDictModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleDict({\n",
    "            \"branch1\": nn.Linear(4, 8),\n",
    "            \"branch2\": nn.Linear(4, 8)\n",
    "        })\n",
    "        self.output: nn.Linear = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, branch_name: str = \"branch1\") -> torch.Tensor:\n",
    "        x = self.branches[branch_name](x)\n",
    "        return self.output(x)\n",
    "\n",
    "md_model = ModuleDictModel()\n",
    "output_md = md_model(x, branch_name=\"branch2\")\n",
    "print(\"nn.ModuleDict output shape:\", output_md.shape)"
   ],
   "metadata": {
    "id": "lcBUsJWo6DNC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Homework\n",
    "\n",
    "2 задания:\n",
    "1. Реализуйте требуемый в заголовке блок (максмсум 0.8 балов)."
   ],
   "metadata": {
    "id": "MzrEd6ee7B_-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Block (0.1 балл)\n",
    "\n",
    "![Resnet](assets/ResBlock.png)\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385"
   ],
   "metadata": {
    "id": "RSnNdPvwojOR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ..."
   ],
   "metadata": {
    "id": "iYGabO4holxg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Depthwise Separable Convolution (0.1 балл)\n",
    "![DepthWiseConv](assets/DepthWiseConv.png)\n",
    "\n",
    "https://arxiv.org/pdf/1610.02357"
   ],
   "metadata": {
    "id": "Db0ghrXnvbGL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ..."
   ],
   "metadata": {
    "id": "8gtIxVJYvdZ8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vanilla Attention (0.1 балл)\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "\\text{query} \\in \\mathbb{R}^{B \\times d} \\\\\n",
    "\\text{key} \\in \\mathbb{R}^{B \\times L \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Alignment Scores\n",
    "\n",
    "$$\n",
    "\\text{score} = \\text{key} \\cdot (W_\\text{align} \\, \\text{query})^T \\\\\n",
    "\\text{score} \\in \\mathbb{R}^{B \\times L}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Weights\n",
    "\n",
    "$$\n",
    "\\text{att} = \\text{softmax}(\\text{score}, \\text{dim}=1) \\\\\n",
    "\\text{att} \\in \\mathbb{R}^{B \\times L}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Context Vector\n",
    "\n",
    "$$\n",
    "\\text{context} = \\sum_{i=1}^{L} \\text{att}_i \\cdot \\text{key}_i \\\\\n",
    "\\text{context} \\in \\mathbb{R}^{B \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "$$\n",
    "\\text{out} = \\tanh(W_\\text{value} \\, \\text{context} + W_\\text{query} \\, \\text{query}) \\\\\n",
    "\\text{out} \\in \\mathbb{R}^{B \\times d}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1409.0473\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1508.04025"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class VanillaAttention(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dot Product Attention (0.1 балл)\n",
    "\n",
    "$$\n",
    "Q \\in \\mathbb{R}^{B \\times L_q \\times d_k} \\\\\n",
    "K \\in \\mathbb{R}^{B \\times L_k \\times d_k} \\\\\n",
    "V \\in \\mathbb{R}^{B \\times L_k \\times d_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(S, \\text{dim}=-1) \\, V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n"
   ],
   "metadata": {
    "id": "wCKxuu3Uv53Z"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multihead Attention (0.1 балл)\n",
    "\n",
    "![MultiheadAttention](assets/MultiheadAttention.webp)\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n"
   ],
   "metadata": {
    "id": "ABh2R2CQz4lo"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer Encoder Layer (0.1 балл)\n",
    "\n",
    "\n",
    "![Transformer Encoder Layer](assets/TransformerEncoder.png)\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1706.03762"
   ],
   "metadata": {
    "id": "SY_jWpBGxhiC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ...\n"
   ],
   "metadata": {
    "id": "KspOnliUx3Oa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Mixer (0.1 балл)\n",
    "\n",
    "\n",
    "![MLPMixer](assets/MLPMixer.png)\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/2105.01601"
   ],
   "metadata": {
    "id": "8ZfjA0B5vfSs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class MLPMixerBlock(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ...\n"
   ],
   "metadata": {
    "id": "JH9sJ-rKvhll"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConvMixer (0.1 балл)\n",
    "\n",
    "![ConvMixer](assets/ConvMixer.png)\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/2201.09792"
   ],
   "metadata": {
    "id": "CYwqCxCi8JON"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ConvMixer(nn.Module):\n",
    "\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ...):\n",
    "        ...\n"
   ],
   "metadata": {
    "id": "3FIWDq6O8ymg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вопрос (0.2 балла)\n",
    "\n",
    "Объясните, почему MLPMixer, ConvMixer может работать почти так же эффективно, как обычный Multihead Attention.\n",
    "\n",
    "Напишите формулу, связывающую Multihead Attention, ConvMixer и MLPMixer\n",
    "\n",
    "Опишите преимущества и недостатки между ConvMixer, MLPMixer и Multihead Attention\n",
    "\n",
    "---\n",
    "\n",
    "Ответ: ..."
   ],
   "metadata": {
    "id": "L3IbQlkr80_M"
   }
  }
 ]
}
