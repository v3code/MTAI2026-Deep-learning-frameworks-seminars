{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Seminar 1: Introduction to PyTorch",
   "metadata": {
    "id": "i_JI8pONGEKn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source machine learning framework used for:\n",
    "- numerical computation with tensors\n",
    "- building and training neural networks\n",
    "- automatic differentiation (autograd)\n",
    "\n",
    "It is widely used in research and production because it is:\n",
    "- easy to use and Pythonic\n",
    "- flexible (dynamic computation graphs)\n",
    "- efficient on both CPUs and GPUs\n",
    "\n",
    "For automatic differentiation PyTorch uses computational graph\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Computation Graph?\n",
    "\n",
    "A computation graph is a directed graph that represents:\n",
    "- **nodes** → tensors (data)\n",
    "- **edges** → operations applied to those tensors\n",
    "\n",
    "PyTorch builds this graph **dynamically at runtime** as operations are executed.\n",
    "\n",
    "Computation graphs allow PyTorch to:\n",
    "- record every operation applied to tensors during the forward pass\n",
    "- understand how each output is connected to its inputs\n",
    "- automatically compute gradients using backpropagation\n",
    "\n",
    "Because this graph is available, PyTorch can efficiently apply the chain rule\n",
    "to propagate gradients backward through complex computations.\n",
    "\n",
    "As a result, calling `.backward()` on a final tensor automatically computes\n",
    "all required gradients without manually deriving them.\n"
   ],
   "metadata": {
    "id": "eO__TAb5cilv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchviz"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4B5Tk7tchWM",
    "outputId": "c72e0732-a732-44df-8140-faa822f02d0f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def show_tensor_params(*tensors):\n",
    "  for x in tensors:\n",
    "    print('---')\n",
    "    print(f\"data - {x.data}\")\n",
    "    print(f\"storage - {x.storage()}\")\n",
    "    print(f\"grad_fn - {x.grad_fn}\")\n",
    "    print(f\"req_grad - {x.requires_grad}\")\n",
    "    print(f\"is_leaf - {x.is_leaf}\")\n",
    "    print(f\"version - {x._version}\")"
   ],
   "metadata": {
    "id": "_FAMas_JGNAJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "__SpzN3HFlGi"
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "In this example:\n",
    "- `x` and `y` are input tensors (leaf nodes)\n",
    "- operations like multiplication or additions create graph nodes\n",
    "- `z` is the final output tensor\n",
    "\n",
    "The visualized graph shows how data flows forward and how gradients will flow backward during training."
   ],
   "metadata": {
    "id": "66aKnVemecQa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Create tensors\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Simple operations\n",
    "z = x * y + y + 2\n",
    "\n",
    "# Visualize computation graph\n",
    "dot = make_dot(z, params={\"x\": x, \"y\": y})\n",
    "dot\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "evuQ-HXReYk_",
    "outputId": "23906271-638d-4c90-af8c-bb60e83e7c58"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "z.backward()\n",
    "x.grad"
   ],
   "metadata": {
    "id": "HuohekZ4jGc8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor in computation graph"
   ],
   "metadata": {
    "id": "56jHtxYkfQ3I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor(5.0)\n",
    "show_tensor_params(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1E7wniPuGPDX",
    "outputId": "5b349907-3c36-4498-890d-afdf1d6f5f4a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **data**\n",
    "  - The actual numerical value(s) stored in the tensor\n",
    "  - For a scalar tensor, this is a single number (e.g., `5.0`)\n",
    "  - For higher-dimensional tensors, this is an array of values\n",
    "\n",
    "- **storage**\n",
    "  - The low-level memory buffer that holds the tensor’s data\n",
    "  - Contains information about:\n",
    "    - data type (`dtype`, e.g., `float32`)\n",
    "    - device (`cpu` or `cuda`)\n",
    "    - number of elements stored\n",
    "  - Multiple tensors (such as views or slices) can share the same storage\n",
    "\n",
    "- **grad_fn**\n",
    "  - The function that created the tensor during an autograd-tracked operation\n",
    "  - `None` means the tensor was created directly by the user\n",
    "  - When present, it represents a node in the computation graph\n",
    "\n",
    "- **requires_grad**\n",
    "  - Indicates whether PyTorch should track operations on this tensor for gradient computation\n",
    "  - If `False`, no gradients are computed\n",
    "  - If `True`, gradients are computed during backpropagation\n",
    "\n",
    "- **is_leaf**\n",
    "  - Indicates whether the tensor is a leaf node in the computation graph\n",
    "  - Leaf tensors are created directly by the user, not by operations\n",
    "  - Only leaf tensors can have gradients stored in the `.grad` attribute\n",
    "\n",
    "- **version**\n",
    "  - Internal counter that tracks in-place modifications to the tensor\n",
    "  - Incremented whenever the tensor is modified in-place (e.g., `+=`, `.add_()`)\n",
    "  - Used by autograd to ensure correct gradient computation\n"
   ],
   "metadata": {
    "id": "r3R0OBJDGVj-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x += 1"
   ],
   "metadata": {
    "id": "1qDH_9KdG2-_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "show_tensor_params(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQQyxwiWIDD7",
    "outputId": "8e8e1d0c-0bcf-4ede-a455-03199cd6b492"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagation and Graph Deletion\n",
    "\n",
    "In PyTorch, the computation graph is **freed after `.backward()` is called**.\n",
    "This is done to save memory.\n",
    "\n",
    "That means:\n",
    "- After one backward pass, the graph no longer exists\n",
    "- Calling `.backward()` again on the same graph will raise an error\n",
    "\n",
    "---\n",
    "\n",
    "### How to Keep the Graph and Use Multiple Backward Passes\n",
    "\n",
    "To **prevent the graph from being deleted**, you must explicitly tell PyTorch to keep it.\n",
    "\n",
    "You should keep the graph when:\n",
    "- You need to run `.backward()` **multiple times on the same output**\n",
    "- You want to reuse the same forward computation for multiple gradient passes\n",
    "- You are inspecting or debugging gradients step by step\n",
    "\n",
    "Using `retain_graph=True` means:\n",
    "> “Keep the computation graph alive after backpropagation so it can be reused.”\n"
   ],
   "metadata": {
    "id": "zM8pUdgDlfK4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z.backward()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "ACIrLPbWa4am",
    "outputId": "76535983-d5aa-4578-97e0-413a90fa1409"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "z = x * y + y + 2"
   ],
   "metadata": {
    "id": "edhm5CJNmV93"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "z.backward(retain_graph=True)"
   ],
   "metadata": {
    "id": "NaZ8knPEmZrJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "z.backward(retain_graph=True)"
   ],
   "metadata": {
    "id": "1jgA28e0mZ5X"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using `.retain_grad()` on Intermediate Tensors\n",
    "\n",
    "Sometimes you want gradients **not only for leaf tensors**, but also for **intermediate tensors** in the computation graph.  \n",
    "\n",
    "By default:\n",
    "- Only leaf tensors store gradients in `.grad`\n",
    "- Intermediate tensors’ gradients are discarded after `.backward()`\n",
    "\n",
    "You can call `.retain_grad()` on an intermediate tensor to **keep its gradient**, allowing you to inspect it later without keeping the whole graph alive with `retain_graph=True`.\n",
    "\n",
    "This is particularly useful for:\n",
    "- Debugging deep networks\n",
    "- Visualizing gradients of hidden layers\n"
   ],
   "metadata": {
    "id": "GZxURhGrnS52"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Create input tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Intermediate computation\n",
    "a = x * 3\n",
    "a.retain_grad()  # Keep gradient for this intermediate tensor\n",
    "b = a ** 2\n",
    "c = b + 1\n",
    "\n",
    "# Backward pass\n",
    "c.backward()\n",
    "\n",
    "# Gradients\n",
    "print(\"Gradient of x:\", x.grad)   # leaf tensor\n",
    "print(\"Gradient of a:\", a.grad)   # intermediate tensor retained manually\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtTnxJNHnYtP",
    "outputId": "9fb0d446-ea3c-4848-b5fc-51a09309f382"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Context for backpropagation"
   ],
   "metadata": {
    "id": "xkbgIacLLWBh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class Exp(Function):\n",
    "  \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, i):\n",
    "    \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "    \"\"\"\n",
    "    result = i.exp()\n",
    "    ctx.save_for_backward(result)\n",
    "    return result\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "    \"\"\"\n",
    "    print(ctx.saved_tensors)\n",
    "    result, = ctx.saved_tensors\n",
    "    return grad_output * result"
   ],
   "metadata": {
    "id": "jnFVllwzLdSU"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "output = Exp.apply(input)\n",
    "output"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2AJ6b-xM1u3",
    "outputId": "dfe775e2-a69e-4352-f48c-311f11b2382a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "output.backward()\n",
    "show_tensor_params(output)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IndRnAN1M6l_",
    "outputId": "3568be9a-3f17-48e8-f7a8-9ecd89f7ab9a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using `.detach()` to Break a Tensor from the Computation Graph\n",
    "\n",
    "Sometimes we want to **stop gradients from flowing** through a tensor.  \n",
    "This is useful when:\n",
    "- You want to use a tensor in computations **without tracking it for autograd**\n",
    "- You want to avoid memory usage from building unnecessary computation graphs\n",
    "- You are performing intermediate calculations that **should not affect gradients**\n",
    "\n",
    "`.detach()` creates a **new tensor that shares data** but **does not track gradients**.\n"
   ],
   "metadata": {
    "id": "yYhe4yD5tdBw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With `.detach()`"
   ],
   "metadata": {
    "id": "iki2yqRjuguV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "\n",
    "a1 = x * y\n",
    "b1 = a1 ** 2\n",
    "c1 = a1 + b1\n",
    "\n",
    "print(\"Computation graph WITHOUT detach:\")\n",
    "dot1 = make_dot(c1, params={\"x\": x, \"y\": y})\n",
    "dot1.render(\"graph_without_detach\", format=\"png\")  # saves as PNG\n",
    "dot1\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "t_d8OUQlt2Ci",
    "outputId": "eabadd71-8b30-4bf3-d3fb-2641f3d2d0e7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "c1.backward(retain_graph=True)\n",
    "print(\"Gradients WITHOUT detach:\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKc_0xMHutZF",
    "outputId": "0809e2a8-20f1-444c-97a5-84befea9f423"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Without `.detach()`"
   ],
   "metadata": {
    "id": "V0ylxdb5uzak"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "a2 = x * y\n",
    "b2 = a2.detach() ** 2  # detach a2\n",
    "c2 = a2 + b2\n",
    "\n",
    "\n",
    "print(\"\\nComputation graph WITH detach:\")\n",
    "dot2 = make_dot(c2, params={\"x\": x, \"y\": y})\n",
    "dot2.render(\"graph_with_detach\", format=\"png\")\n",
    "dot2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Qx426sIcu1SQ",
    "outputId": "2b397c64-27d2-4d0c-96a8-8667204bc61f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "c2.backward()\n",
    "print(\"Gradients WITH detach:\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0EAXGMBvFB-",
    "outputId": "8958df43-142a-405a-ff50-26edd19222c8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "JmA4ipcGn9Ow"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a Simple Neural Network on MNIST with PyTorch"
   ],
   "metadata": {
    "id": "Y7Jv-CZ0rZ5I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is `nn.Module`?\n",
    "\n",
    "`nn.Module` is the base class for all neural networks in PyTorch.\n",
    "\n",
    "It provides:\n",
    "- a structured way to define layers and parameters\n",
    "- automatic registration of trainable weights\n",
    "- integration with PyTorch’s autograd system\n",
    "\n",
    "Any custom neural network must inherit from `nn.Module`\n",
    "and implement a `forward` method.\n"
   ],
   "metadata": {
    "id": "49cXOkNsn2K7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJhNI8Nv6FIO",
    "outputId": "92594ad2-3f08-402e-b83f-caf69e4caf12"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is a Dataset and DataLoader?\n",
    "\n",
    "A `Dataset` represents a collection of samples and labels.\n",
    "\n",
    "A `DataLoader`:\n",
    "- loads data in mini-batches\n",
    "- shuffles data each epoch\n",
    "- handles parallel data loading\n",
    "\n",
    "Together, they allow efficient and scalable training."
   ],
   "metadata": {
    "id": "QVYg-lGEoFPW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n"
   ],
   "metadata": {
    "id": "THgM5aBVrkcC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "de45ba5c-239f-4fb9-f5b6-8d57ec8a19d4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Functions in PyTorch\n",
    "\n",
    "A loss function (also called a criterion) measures how well the model’s\n",
    "predictions match the true targets.\n",
    "\n",
    "In PyTorch, loss functions are provided in the `torch.nn` module and are\n",
    "implemented as **callable objects**. They take:\n",
    "- the model’s output (predictions)\n",
    "- the ground-truth labels\n",
    "\n",
    "and return a **single scalar value** representing the error.\n",
    "\n",
    "This scalar loss is the starting point for backpropagation.\n",
    "\n",
    "In addition to module-based loss functions, PyTorch also provides functional-style losses in the `torch.nn.functional` package.\n",
    "\n",
    "### How Loss Functions Fit into Training\n",
    "\n",
    "During training:\n",
    "1. The model produces predictions in the forward pass\n",
    "2. The loss function compares predictions with targets\n",
    "3. Calling `.backward()` on the loss computes gradients for all parameters\n",
    "\n",
    "The loss function defines **what the model is trying to optimize**.\n"
   ],
   "metadata": {
    "id": "YZq50RYioQSd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "id": "XEBreCz_rnWb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers in PyTorch\n",
    "\n",
    "An optimizer updates the model’s parameters using the gradients computed\n",
    "during backpropagation.\n",
    "\n",
    "PyTorch optimizers live in the `torch.optim` module and operate on:\n",
    "- model parameters (weights and biases)\n",
    "- gradients stored in `.grad` fields\n",
    "\n",
    "Optimizers implement different strategies for adjusting parameters to\n",
    "minimize the loss.\n",
    "\n",
    "### Common Optimizers in PyTorch\n",
    "\n",
    "Some commonly used optimizers include:\n",
    "\n",
    "- `optim.SGD`\n",
    "  - Stochastic Gradient Descent\n",
    "  - Simple and widely used\n",
    "  - Can include momentum for faster convergence\n",
    "\n",
    "- `optim.Adam`\n",
    "  - Adaptive learning rate optimizer\n",
    "  - Works well for many deep learning problems\n",
    "  - Often a good default choice\n",
    "\n",
    "- `optim.RMSprop`\n",
    "  - Uses moving averages of gradients\n",
    "  - Common in recurrent and reinforcement learning settings\n",
    "\n",
    "- `optim.Adagrad`\n",
    "  - Adapts learning rates based on parameter updates\n",
    "  - Useful for sparse data\n",
    "\n",
    "### How Optimizers Work in Training\n",
    "\n",
    "A typical training step looks like this:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Call `.backward()` to compute gradients\n",
    "3. Call `optimizer.step()` to update parameters\n",
    "4. Call `optimizer.zero_grad()` to clear old gradients\n",
    "\n",
    "Gradients accumulate by default, so clearing them each step is important.\n",
    "\n",
    "### Why Optimizers Are Separate from Models\n",
    "\n",
    "Keeping optimizers separate from the model allows:\n",
    "- swapping optimization strategies easily\n",
    "- using different learning rates or optimizers for different parameters\n",
    "- clean separation between model definition and training logic\n",
    "\n",
    "The optimizer defines **how the model learns**, while the loss defines\n",
    "**what the model learns**.\n"
   ],
   "metadata": {
    "id": "0J0HNwa6p5Kh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ],
   "metadata": {
    "id": "x4uzUlyEqrBk"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What Happens During Training?\n",
    "\n",
    "Each training step consists of:\n",
    "1. Forward pass through the network\n",
    "2. Loss computation\n",
    "3. Gradient computation using backpropagation\n",
    "4. Parameter update using the optimizer\n"
   ],
   "metadata": {
    "id": "Cox4WlW4u6RH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umihSWP7qu3H",
    "outputId": "2a782efb-d8d2-4ce9-8fe7-aca356ea87bb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Custom Datasets in PyTorch\n",
    "\n",
    "In addition to built-in datasets, PyTorch allows you to define **custom datasets**\n",
    "by subclassing `torch.utils.data.Dataset`.\n",
    "\n",
    "To create a custom dataset, you need to implement:\n",
    "- `__len__()` → returns the total number of samples\n",
    "- `__getitem__(idx)` → returns one sample (and its label) given an index\n",
    "\n",
    "This approach gives full control over how data is loaded, transformed,\n",
    "and returned to the training loop.\n"
   ],
   "metadata": {
    "id": "WkA1fkV8vT8Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "# Example usage\n",
    "data = torch.randn(10, 3)\n",
    "labels = torch.randint(0, 2, (10,))\n",
    "\n",
    "dataset = SimpleDataset(data, labels)\n",
    "print(len(dataset))\n",
    "print(dataset[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkCbeesvvWXQ",
    "outputId": "aaf249da-c034-41af-a244-d59415c5756d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Homework"
   ],
   "metadata": {
    "id": "vyWaAcrRp58P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Реализовать операции по графу (максимум 0.2 балла)\n"
   ],
   "metadata": {
    "id": "t0Yb3kCAvvVc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 1.1 (0.05 балл)\n",
    "\n",
    "![task1.1](assets/task_1.png)"
   ],
   "metadata": {
    "id": "ggn_fPOjxMsF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "c = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Operations\n",
    "...\n",
    "\n",
    "# Visualize graph\n",
    "dot = make_dot(z, params={\"x\": x, \"y\": y, \"c\": c})\n",
    "\n",
    "dot\n",
    "\n"
   ],
   "metadata": {
    "id": "s84yMWAuvEGI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "outputId": "29921af9-6aa7-4bd8-d5f9-b2570e0c4cbf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 1.2 (0.05 балл)\n",
    "\n",
    "\n",
    "![task1.2](assets/task_2.png)\n"
   ],
   "metadata": {
    "id": "TIJOAdABysl2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.tensor(1.5, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "exp_val = torch.tensor(2.0, requires_grad=True)  # tensor for exponent\n",
    "\n",
    "# Operations\n",
    "...\n",
    "\n",
    "# Visualize graph\n",
    "dot = make_dot(z, params={\"x\": x, \"y\": y, \"exp_val\": exp_val})\n",
    "\n",
    "dot\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "PKzSxESVymtX",
    "outputId": "bc1e6849-cf79-4eab-dd3a-f5d81338f0d7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 1.3 (0.05 балл)\n",
    "\n",
    "\n",
    "\n",
    "![task1.3](assets/task_3.png)\n"
   ],
   "metadata": {
    "id": "tHnNqX1VzAty"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "c = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Operations\n",
    "...\n",
    "\n",
    "# Visualize graph\n",
    "dot = make_dot(z, params={\"x\": x, \"y\": y, \"c\": c})\n",
    "\n",
    "dot\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2A5phciKzDDL",
    "outputId": "d063b2a4-066f-4570-96e0-dc9f880fdd12"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 1.4 (0.05 балл)\n",
    "\n",
    "\n",
    "![task1.4](assets/task_4.png)\n"
   ],
   "metadata": {
    "id": "LuTRypLczGS4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "c1 = torch.tensor(1.0, requires_grad=True)\n",
    "c2 = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Operations\n",
    "...\n",
    "\n",
    "# Visualize graph\n",
    "dot = make_dot(z, params={\"x\": x, \"c1\": c1, \"c2\": c2})\n",
    "\n",
    "dot\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Zv3RUlHAzMbS",
    "outputId": "b274aa60-af1c-49a5-efd2-2b37bf904b59"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Реализовать функции для автоматического дифференциирования (максимум 0.4 балла)\n",
    "\n"
   ],
   "metadata": {
    "id": "eagv5_QA2t6z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задача 2.1: Функция Sinc (0.2 балла)\n",
    "\n",
    "Реализуйте кастомную `torch.autograd.Function`, которая вычисляет:\n",
    "\n",
    "$$\n",
    "\\text{sinc}(x) = \\frac{\\sin(x)}{x}, \\quad x \\neq 0, \\quad \\text{sinc}(0) = 1\n",
    "$$\n"
   ],
   "metadata": {
    "id": "SgoCXTgn6OBk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class SincFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ...\n",
    "\n",
    "# ----------------------------\n",
    "# Тестирование\n",
    "# ----------------------------\n",
    "x = torch.tensor([0.0, 1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = SincFunction.apply(x)\n",
    "y.sum().backward()\n",
    "\n",
    "expected_values = torch.tensor([1.0000, 0.8415, 0.4546, 0.0470])\n",
    "expected_grads = torch.tensor([0.0000, -0.3012, -0.4354, -0.3457])\n",
    "\n",
    "assert torch.allclose(y, expected_values, atol=1e-4), \"Forward Sinc рассчитан неверно\"\n",
    "assert torch.allclose(x.grad, expected_grads, atol=1e-4), \"Градиенты Sinc рассчитаны неверно\"\n",
    "\n",
    "print(\"Задача 2.1: все тесты пройдены ✅\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq9MKh-e6NRH",
    "outputId": "3248979b-8cea-4af5-b66a-6f37e0d6faaa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задача 2.2: Функция Гаусса (0.2 балла)\n",
    "\n",
    "Реализуйте кастомную `torch.autograd.Function`, которая вычисляет:\n",
    "\n",
    "$$\n",
    "f(x) = e^{-x^2}\n",
    "$$"
   ],
   "metadata": {
    "id": "RwH4O7IT6yUp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GaussianFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ...\n",
    "\n",
    "# ----------------------------\n",
    "# Тестирование\n",
    "# ----------------------------\n",
    "x = torch.tensor([-1.0, 0.0, 0.5, 1.0], requires_grad=True)\n",
    "y = GaussianFunction.apply(x)\n",
    "y.sum().backward()\n",
    "\n",
    "expected_values = torch.tensor([0.3679, 1.0000, 0.7788, 0.3679])\n",
    "expected_grads = torch.tensor([ 0.7358, -0.0000, -0.7788, -0.7358])\n",
    "\n",
    "\n",
    "assert torch.allclose(y, expected_values, atol=1e-4), \"Forward Гаусса рассчитан неверно\"\n",
    "assert torch.allclose(x.grad, expected_grads, atol=1e-4), \"Градиенты Гаусса рассчитаны неверно\"\n",
    "\n",
    "print(\"Задача 2.2: все тесты пройдены ✅\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Br4-LyAK7GBO",
    "outputId": "9216cc02-cc94-489e-f894-a9f2d22ea2f0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задача 3: Реализация пользовательского Dataset синуса и косинуса. (0.1 балл)\n",
    "\n",
    "Реализуйте свой класс, наследующий `torch.utils.data.Dataset`.\n",
    "\n",
    "- Для каждого индекса `i` возвращайте пару `(sin(i), cos(i))`\n",
    "- Размер датасета задается при инициализации\n",
    "- Проверьте реализацию с помощью `assert`:\n",
    "  - длина датасета совпадает с указанной\n",
    "  - значения синуса и косинуса соответствуют ожидаемым\n"
   ],
   "metadata": {
    "id": "qre-EMHe8SGG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "class SinCosDataset(Dataset):\n",
    "    ...\n",
    "\n",
    "# ----------------------------\n",
    "# Тестирование\n",
    "# ----------------------------\n",
    "dataset_size = 10\n",
    "dataset = SinCosDataset(dataset_size)\n",
    "\n",
    "# 1. Проверяем длину\n",
    "assert len(dataset) == dataset_size, f\"Expected dataset length {dataset_size}, got {len(dataset)}\"\n",
    "\n",
    "# 2. Проверяем значения для нескольких индексов\n",
    "for i in range(dataset_size):\n",
    "    x, y = dataset[i]\n",
    "    expected_x = math.sin(i)\n",
    "    expected_y = math.cos(i)\n",
    "    assert math.isclose(x.item(), expected_x, rel_tol=1e-6), f\"sin({i}) expected {expected_x}, got {x.item()}\"\n",
    "    assert math.isclose(y.item(), expected_y, rel_tol=1e-6), f\"cos({i}) expected {expected_y}, got {y.item()}\"\n",
    "\n",
    "print(\"Dataset tests passed ✅\")\n"
   ],
   "metadata": {
    "id": "tF0zbTiL8WS1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задача 4. (0.1 балл)\n",
    "Вопрос: В каком случае нам полезно использовать накопление градиентов?\n",
    "\n",
    "Ответ: ..."
   ],
   "metadata": {
    "id": "O0MfitH281ul"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задача 5. (0.2 балла)\n",
    "### Реализуйте обучение модели на мнист из примера выше на GPU (0.1 балла)\n",
    "Для того чтобы перевести модель или тензор на GPU можно использовать метод `.cuda()`. Чтобы перевести на CPU - метод `.cpu()`.\n",
    "\n",
    "### Доп.задание (0.1 балл): Реализуйте ответ из задачи 4"
   ],
   "metadata": {
    "id": "_N9Nqdgk9iFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "..."
   ],
   "metadata": {
    "id": "k61rXbAh8wfL"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
